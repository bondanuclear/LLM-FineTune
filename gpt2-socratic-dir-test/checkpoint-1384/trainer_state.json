{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1384,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014450867052023121,
      "grad_norm": 0.6338176727294922,
      "learning_rate": 3.5971223021582732e-06,
      "loss": 3.7833,
      "step": 10
    },
    {
      "epoch": 0.028901734104046242,
      "grad_norm": 0.9328519701957703,
      "learning_rate": 7.1942446043165465e-06,
      "loss": 3.7708,
      "step": 20
    },
    {
      "epoch": 0.04335260115606936,
      "grad_norm": 0.7859657406806946,
      "learning_rate": 1.0791366906474821e-05,
      "loss": 3.7893,
      "step": 30
    },
    {
      "epoch": 0.057803468208092484,
      "grad_norm": 0.8425351977348328,
      "learning_rate": 1.4388489208633093e-05,
      "loss": 3.7633,
      "step": 40
    },
    {
      "epoch": 0.07225433526011561,
      "grad_norm": 0.8633155822753906,
      "learning_rate": 1.7985611510791367e-05,
      "loss": 3.6982,
      "step": 50
    },
    {
      "epoch": 0.08670520231213873,
      "grad_norm": 0.9733292460441589,
      "learning_rate": 2.1582733812949642e-05,
      "loss": 3.8193,
      "step": 60
    },
    {
      "epoch": 0.10115606936416185,
      "grad_norm": 1.0403269529342651,
      "learning_rate": 2.5179856115107914e-05,
      "loss": 3.765,
      "step": 70
    },
    {
      "epoch": 0.11560693641618497,
      "grad_norm": 1.0769615173339844,
      "learning_rate": 2.8776978417266186e-05,
      "loss": 3.6991,
      "step": 80
    },
    {
      "epoch": 0.13005780346820808,
      "grad_norm": 1.3194289207458496,
      "learning_rate": 3.237410071942446e-05,
      "loss": 3.6828,
      "step": 90
    },
    {
      "epoch": 0.14450867052023122,
      "grad_norm": 1.6155569553375244,
      "learning_rate": 3.597122302158273e-05,
      "loss": 3.6552,
      "step": 100
    },
    {
      "epoch": 0.15895953757225434,
      "grad_norm": 1.0198220014572144,
      "learning_rate": 3.956834532374101e-05,
      "loss": 3.4803,
      "step": 110
    },
    {
      "epoch": 0.17341040462427745,
      "grad_norm": 1.3547694683074951,
      "learning_rate": 4.3165467625899284e-05,
      "loss": 3.5782,
      "step": 120
    },
    {
      "epoch": 0.18786127167630057,
      "grad_norm": 1.3499767780303955,
      "learning_rate": 4.676258992805755e-05,
      "loss": 3.4373,
      "step": 130
    },
    {
      "epoch": 0.2023121387283237,
      "grad_norm": 1.317399263381958,
      "learning_rate": 4.995983935742972e-05,
      "loss": 3.4704,
      "step": 140
    },
    {
      "epoch": 0.21676300578034682,
      "grad_norm": 1.134781837463379,
      "learning_rate": 4.955823293172691e-05,
      "loss": 3.4057,
      "step": 150
    },
    {
      "epoch": 0.23121387283236994,
      "grad_norm": 0.9982414841651917,
      "learning_rate": 4.9156626506024104e-05,
      "loss": 3.3473,
      "step": 160
    },
    {
      "epoch": 0.24566473988439305,
      "grad_norm": 0.9833696484565735,
      "learning_rate": 4.875502008032129e-05,
      "loss": 3.2728,
      "step": 170
    },
    {
      "epoch": 0.26011560693641617,
      "grad_norm": 1.047238826751709,
      "learning_rate": 4.8353413654618476e-05,
      "loss": 3.3043,
      "step": 180
    },
    {
      "epoch": 0.2745664739884393,
      "grad_norm": 0.9287770986557007,
      "learning_rate": 4.7951807228915665e-05,
      "loss": 3.2344,
      "step": 190
    },
    {
      "epoch": 0.28901734104046245,
      "grad_norm": 0.8354200124740601,
      "learning_rate": 4.7550200803212854e-05,
      "loss": 3.194,
      "step": 200
    },
    {
      "epoch": 0.30346820809248554,
      "grad_norm": 0.8808484673500061,
      "learning_rate": 4.714859437751004e-05,
      "loss": 3.1977,
      "step": 210
    },
    {
      "epoch": 0.3179190751445087,
      "grad_norm": 0.7792506217956543,
      "learning_rate": 4.674698795180723e-05,
      "loss": 3.0889,
      "step": 220
    },
    {
      "epoch": 0.33236994219653176,
      "grad_norm": 0.7389611005783081,
      "learning_rate": 4.634538152610442e-05,
      "loss": 3.1399,
      "step": 230
    },
    {
      "epoch": 0.3468208092485549,
      "grad_norm": 0.871417760848999,
      "learning_rate": 4.594377510040161e-05,
      "loss": 3.0631,
      "step": 240
    },
    {
      "epoch": 0.36127167630057805,
      "grad_norm": 0.8647934198379517,
      "learning_rate": 4.55421686746988e-05,
      "loss": 3.0262,
      "step": 250
    },
    {
      "epoch": 0.37572254335260113,
      "grad_norm": 0.8375223875045776,
      "learning_rate": 4.514056224899599e-05,
      "loss": 3.1178,
      "step": 260
    },
    {
      "epoch": 0.3901734104046243,
      "grad_norm": 0.8261134624481201,
      "learning_rate": 4.473895582329318e-05,
      "loss": 3.0482,
      "step": 270
    },
    {
      "epoch": 0.4046242774566474,
      "grad_norm": 0.8245840668678284,
      "learning_rate": 4.433734939759036e-05,
      "loss": 2.9929,
      "step": 280
    },
    {
      "epoch": 0.4190751445086705,
      "grad_norm": 0.873359203338623,
      "learning_rate": 4.393574297188755e-05,
      "loss": 2.9308,
      "step": 290
    },
    {
      "epoch": 0.43352601156069365,
      "grad_norm": 0.9003682732582092,
      "learning_rate": 4.353413654618474e-05,
      "loss": 3.0255,
      "step": 300
    },
    {
      "epoch": 0.4479768786127168,
      "grad_norm": 0.9605580568313599,
      "learning_rate": 4.313253012048193e-05,
      "loss": 3.0321,
      "step": 310
    },
    {
      "epoch": 0.4624277456647399,
      "grad_norm": 0.9550971984863281,
      "learning_rate": 4.273092369477912e-05,
      "loss": 2.9538,
      "step": 320
    },
    {
      "epoch": 0.476878612716763,
      "grad_norm": 0.9975482225418091,
      "learning_rate": 4.232931726907631e-05,
      "loss": 2.8506,
      "step": 330
    },
    {
      "epoch": 0.4913294797687861,
      "grad_norm": 0.930985152721405,
      "learning_rate": 4.1927710843373496e-05,
      "loss": 2.9498,
      "step": 340
    },
    {
      "epoch": 0.5057803468208093,
      "grad_norm": 0.9944967031478882,
      "learning_rate": 4.1526104417670686e-05,
      "loss": 2.9165,
      "step": 350
    },
    {
      "epoch": 0.5202312138728323,
      "grad_norm": 1.0651510953903198,
      "learning_rate": 4.1124497991967875e-05,
      "loss": 2.8466,
      "step": 360
    },
    {
      "epoch": 0.5346820809248555,
      "grad_norm": 1.0028268098831177,
      "learning_rate": 4.0722891566265064e-05,
      "loss": 2.9349,
      "step": 370
    },
    {
      "epoch": 0.5491329479768786,
      "grad_norm": 0.9597352147102356,
      "learning_rate": 4.0321285140562246e-05,
      "loss": 2.8264,
      "step": 380
    },
    {
      "epoch": 0.5635838150289018,
      "grad_norm": 0.9460840821266174,
      "learning_rate": 3.9919678714859436e-05,
      "loss": 2.881,
      "step": 390
    },
    {
      "epoch": 0.5780346820809249,
      "grad_norm": 1.0761054754257202,
      "learning_rate": 3.9518072289156625e-05,
      "loss": 2.9015,
      "step": 400
    },
    {
      "epoch": 0.5924855491329479,
      "grad_norm": 1.058117389678955,
      "learning_rate": 3.9116465863453814e-05,
      "loss": 2.8659,
      "step": 410
    },
    {
      "epoch": 0.6069364161849711,
      "grad_norm": 1.1236591339111328,
      "learning_rate": 3.8714859437751e-05,
      "loss": 2.8038,
      "step": 420
    },
    {
      "epoch": 0.6213872832369942,
      "grad_norm": 1.0592775344848633,
      "learning_rate": 3.831325301204819e-05,
      "loss": 2.8727,
      "step": 430
    },
    {
      "epoch": 0.6358381502890174,
      "grad_norm": 1.061248540878296,
      "learning_rate": 3.791164658634538e-05,
      "loss": 2.7977,
      "step": 440
    },
    {
      "epoch": 0.6502890173410405,
      "grad_norm": 1.2862145900726318,
      "learning_rate": 3.751004016064257e-05,
      "loss": 2.8486,
      "step": 450
    },
    {
      "epoch": 0.6647398843930635,
      "grad_norm": 1.2827205657958984,
      "learning_rate": 3.710843373493976e-05,
      "loss": 2.8321,
      "step": 460
    },
    {
      "epoch": 0.6791907514450867,
      "grad_norm": 1.0595139265060425,
      "learning_rate": 3.670682730923695e-05,
      "loss": 2.7776,
      "step": 470
    },
    {
      "epoch": 0.6936416184971098,
      "grad_norm": 1.0735368728637695,
      "learning_rate": 3.630522088353414e-05,
      "loss": 2.7663,
      "step": 480
    },
    {
      "epoch": 0.708092485549133,
      "grad_norm": 0.9604960083961487,
      "learning_rate": 3.590361445783133e-05,
      "loss": 2.7274,
      "step": 490
    },
    {
      "epoch": 0.7225433526011561,
      "grad_norm": 0.9627233147621155,
      "learning_rate": 3.550200803212852e-05,
      "loss": 2.7725,
      "step": 500
    },
    {
      "epoch": 0.7369942196531792,
      "grad_norm": 1.0740562677383423,
      "learning_rate": 3.5100401606425706e-05,
      "loss": 2.772,
      "step": 510
    },
    {
      "epoch": 0.7514450867052023,
      "grad_norm": 1.0622196197509766,
      "learning_rate": 3.4698795180722896e-05,
      "loss": 2.6842,
      "step": 520
    },
    {
      "epoch": 0.7658959537572254,
      "grad_norm": 1.0736061334609985,
      "learning_rate": 3.4297188755020085e-05,
      "loss": 2.7593,
      "step": 530
    },
    {
      "epoch": 0.7803468208092486,
      "grad_norm": 1.1282665729522705,
      "learning_rate": 3.3895582329317274e-05,
      "loss": 2.7507,
      "step": 540
    },
    {
      "epoch": 0.7947976878612717,
      "grad_norm": 1.129516839981079,
      "learning_rate": 3.349397590361446e-05,
      "loss": 2.8157,
      "step": 550
    },
    {
      "epoch": 0.8092485549132948,
      "grad_norm": 1.1708241701126099,
      "learning_rate": 3.309236947791165e-05,
      "loss": 2.7158,
      "step": 560
    },
    {
      "epoch": 0.8236994219653179,
      "grad_norm": 1.1060807704925537,
      "learning_rate": 3.269076305220884e-05,
      "loss": 2.7135,
      "step": 570
    },
    {
      "epoch": 0.838150289017341,
      "grad_norm": 1.1296305656433105,
      "learning_rate": 3.2289156626506024e-05,
      "loss": 2.7814,
      "step": 580
    },
    {
      "epoch": 0.8526011560693642,
      "grad_norm": 1.1117331981658936,
      "learning_rate": 3.1887550200803213e-05,
      "loss": 2.7234,
      "step": 590
    },
    {
      "epoch": 0.8670520231213873,
      "grad_norm": 1.099324107170105,
      "learning_rate": 3.14859437751004e-05,
      "loss": 2.7255,
      "step": 600
    },
    {
      "epoch": 0.8815028901734104,
      "grad_norm": 1.1216002702713013,
      "learning_rate": 3.108433734939759e-05,
      "loss": 2.7118,
      "step": 610
    },
    {
      "epoch": 0.8959537572254336,
      "grad_norm": 1.0290249586105347,
      "learning_rate": 3.068273092369478e-05,
      "loss": 2.6623,
      "step": 620
    },
    {
      "epoch": 0.9104046242774566,
      "grad_norm": 1.1533362865447998,
      "learning_rate": 3.028112449799197e-05,
      "loss": 2.6703,
      "step": 630
    },
    {
      "epoch": 0.9248554913294798,
      "grad_norm": 1.0425360202789307,
      "learning_rate": 2.987951807228916e-05,
      "loss": 2.6676,
      "step": 640
    },
    {
      "epoch": 0.9393063583815029,
      "grad_norm": 1.1484568119049072,
      "learning_rate": 2.947791164658635e-05,
      "loss": 2.6637,
      "step": 650
    },
    {
      "epoch": 0.953757225433526,
      "grad_norm": 1.2296143770217896,
      "learning_rate": 2.9076305220883538e-05,
      "loss": 2.6145,
      "step": 660
    },
    {
      "epoch": 0.9682080924855492,
      "grad_norm": 1.1838223934173584,
      "learning_rate": 2.8674698795180727e-05,
      "loss": 2.6583,
      "step": 670
    },
    {
      "epoch": 0.9826589595375722,
      "grad_norm": 1.3593010902404785,
      "learning_rate": 2.827309236947791e-05,
      "loss": 2.6282,
      "step": 680
    },
    {
      "epoch": 0.9971098265895953,
      "grad_norm": 1.2605606317520142,
      "learning_rate": 2.78714859437751e-05,
      "loss": 2.7071,
      "step": 690
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.487121105194092,
      "eval_runtime": 191.1788,
      "eval_samples_per_second": 5.252,
      "eval_steps_per_second": 0.659,
      "step": 692
    },
    {
      "epoch": 1.0115606936416186,
      "grad_norm": 1.1477229595184326,
      "learning_rate": 2.7469879518072288e-05,
      "loss": 2.6572,
      "step": 700
    },
    {
      "epoch": 1.0260115606936415,
      "grad_norm": 1.3504592180252075,
      "learning_rate": 2.7068273092369477e-05,
      "loss": 2.6635,
      "step": 710
    },
    {
      "epoch": 1.0404624277456647,
      "grad_norm": 1.1302882432937622,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 2.6442,
      "step": 720
    },
    {
      "epoch": 1.0549132947976878,
      "grad_norm": 1.2230939865112305,
      "learning_rate": 2.6265060240963856e-05,
      "loss": 2.6575,
      "step": 730
    },
    {
      "epoch": 1.069364161849711,
      "grad_norm": 1.2685606479644775,
      "learning_rate": 2.5863453815261045e-05,
      "loss": 2.6393,
      "step": 740
    },
    {
      "epoch": 1.083815028901734,
      "grad_norm": 1.22250497341156,
      "learning_rate": 2.5461847389558234e-05,
      "loss": 2.751,
      "step": 750
    },
    {
      "epoch": 1.0982658959537572,
      "grad_norm": 1.2675445079803467,
      "learning_rate": 2.5060240963855423e-05,
      "loss": 2.6297,
      "step": 760
    },
    {
      "epoch": 1.1127167630057804,
      "grad_norm": 1.4295344352722168,
      "learning_rate": 2.4658634538152613e-05,
      "loss": 2.5694,
      "step": 770
    },
    {
      "epoch": 1.1271676300578035,
      "grad_norm": 1.2783763408660889,
      "learning_rate": 2.4257028112449802e-05,
      "loss": 2.5949,
      "step": 780
    },
    {
      "epoch": 1.1416184971098267,
      "grad_norm": 1.262437343597412,
      "learning_rate": 2.385542168674699e-05,
      "loss": 2.7004,
      "step": 790
    },
    {
      "epoch": 1.1560693641618498,
      "grad_norm": 1.3583500385284424,
      "learning_rate": 2.345381526104418e-05,
      "loss": 2.6177,
      "step": 800
    },
    {
      "epoch": 1.1705202312138727,
      "grad_norm": 1.3810867071151733,
      "learning_rate": 2.3052208835341366e-05,
      "loss": 2.6368,
      "step": 810
    },
    {
      "epoch": 1.1849710982658959,
      "grad_norm": 1.4391496181488037,
      "learning_rate": 2.2650602409638555e-05,
      "loss": 2.6544,
      "step": 820
    },
    {
      "epoch": 1.199421965317919,
      "grad_norm": 1.193983793258667,
      "learning_rate": 2.2248995983935745e-05,
      "loss": 2.6264,
      "step": 830
    },
    {
      "epoch": 1.2138728323699421,
      "grad_norm": 1.1536870002746582,
      "learning_rate": 2.1847389558232934e-05,
      "loss": 2.6254,
      "step": 840
    },
    {
      "epoch": 1.2283236994219653,
      "grad_norm": 1.2348408699035645,
      "learning_rate": 2.1445783132530123e-05,
      "loss": 2.6589,
      "step": 850
    },
    {
      "epoch": 1.2427745664739884,
      "grad_norm": 1.154353141784668,
      "learning_rate": 2.104417670682731e-05,
      "loss": 2.5909,
      "step": 860
    },
    {
      "epoch": 1.2572254335260116,
      "grad_norm": 1.2085790634155273,
      "learning_rate": 2.0642570281124498e-05,
      "loss": 2.6123,
      "step": 870
    },
    {
      "epoch": 1.2716763005780347,
      "grad_norm": 1.0920300483703613,
      "learning_rate": 2.0240963855421687e-05,
      "loss": 2.6581,
      "step": 880
    },
    {
      "epoch": 1.2861271676300579,
      "grad_norm": 1.4217627048492432,
      "learning_rate": 1.9839357429718877e-05,
      "loss": 2.6792,
      "step": 890
    },
    {
      "epoch": 1.300578034682081,
      "grad_norm": 1.2221097946166992,
      "learning_rate": 1.9437751004016066e-05,
      "loss": 2.6287,
      "step": 900
    },
    {
      "epoch": 1.3150289017341041,
      "grad_norm": 1.3452926874160767,
      "learning_rate": 1.9036144578313252e-05,
      "loss": 2.5245,
      "step": 910
    },
    {
      "epoch": 1.3294797687861273,
      "grad_norm": 1.3088902235031128,
      "learning_rate": 1.863453815261044e-05,
      "loss": 2.6152,
      "step": 920
    },
    {
      "epoch": 1.3439306358381504,
      "grad_norm": 1.3049513101577759,
      "learning_rate": 1.823293172690763e-05,
      "loss": 2.597,
      "step": 930
    },
    {
      "epoch": 1.3583815028901733,
      "grad_norm": 1.310563564300537,
      "learning_rate": 1.783132530120482e-05,
      "loss": 2.556,
      "step": 940
    },
    {
      "epoch": 1.3728323699421965,
      "grad_norm": 1.3765679597854614,
      "learning_rate": 1.742971887550201e-05,
      "loss": 2.5814,
      "step": 950
    },
    {
      "epoch": 1.3872832369942196,
      "grad_norm": 1.2504135370254517,
      "learning_rate": 1.7028112449799198e-05,
      "loss": 2.5861,
      "step": 960
    },
    {
      "epoch": 1.4017341040462428,
      "grad_norm": 1.3027070760726929,
      "learning_rate": 1.6626506024096387e-05,
      "loss": 2.5686,
      "step": 970
    },
    {
      "epoch": 1.416184971098266,
      "grad_norm": 1.3518542051315308,
      "learning_rate": 1.6224899598393576e-05,
      "loss": 2.5488,
      "step": 980
    },
    {
      "epoch": 1.430635838150289,
      "grad_norm": 1.3284159898757935,
      "learning_rate": 1.5823293172690766e-05,
      "loss": 2.6147,
      "step": 990
    },
    {
      "epoch": 1.4450867052023122,
      "grad_norm": 1.3447898626327515,
      "learning_rate": 1.5421686746987955e-05,
      "loss": 2.6148,
      "step": 1000
    },
    {
      "epoch": 1.4595375722543353,
      "grad_norm": 1.2916176319122314,
      "learning_rate": 1.502008032128514e-05,
      "loss": 2.595,
      "step": 1010
    },
    {
      "epoch": 1.4739884393063583,
      "grad_norm": 1.2881839275360107,
      "learning_rate": 1.461847389558233e-05,
      "loss": 2.6363,
      "step": 1020
    },
    {
      "epoch": 1.4884393063583814,
      "grad_norm": 1.202363133430481,
      "learning_rate": 1.4216867469879519e-05,
      "loss": 2.5773,
      "step": 1030
    },
    {
      "epoch": 1.5028901734104045,
      "grad_norm": 1.1738755702972412,
      "learning_rate": 1.3815261044176708e-05,
      "loss": 2.5021,
      "step": 1040
    },
    {
      "epoch": 1.5173410404624277,
      "grad_norm": 1.1966552734375,
      "learning_rate": 1.3413654618473897e-05,
      "loss": 2.5685,
      "step": 1050
    },
    {
      "epoch": 1.5317919075144508,
      "grad_norm": 1.167253851890564,
      "learning_rate": 1.3012048192771083e-05,
      "loss": 2.5549,
      "step": 1060
    },
    {
      "epoch": 1.546242774566474,
      "grad_norm": 1.2400459051132202,
      "learning_rate": 1.2610441767068274e-05,
      "loss": 2.5858,
      "step": 1070
    },
    {
      "epoch": 1.560693641618497,
      "grad_norm": 1.188104271888733,
      "learning_rate": 1.2208835341365463e-05,
      "loss": 2.5325,
      "step": 1080
    },
    {
      "epoch": 1.5751445086705202,
      "grad_norm": 1.404443383216858,
      "learning_rate": 1.1807228915662651e-05,
      "loss": 2.5634,
      "step": 1090
    },
    {
      "epoch": 1.5895953757225434,
      "grad_norm": 1.4185760021209717,
      "learning_rate": 1.140562248995984e-05,
      "loss": 2.5821,
      "step": 1100
    },
    {
      "epoch": 1.6040462427745665,
      "grad_norm": 1.3758277893066406,
      "learning_rate": 1.100401606425703e-05,
      "loss": 2.6243,
      "step": 1110
    },
    {
      "epoch": 1.6184971098265897,
      "grad_norm": 1.2833665609359741,
      "learning_rate": 1.0602409638554217e-05,
      "loss": 2.5806,
      "step": 1120
    },
    {
      "epoch": 1.6329479768786128,
      "grad_norm": 1.1311546564102173,
      "learning_rate": 1.0200803212851406e-05,
      "loss": 2.5639,
      "step": 1130
    },
    {
      "epoch": 1.647398843930636,
      "grad_norm": 1.3042768239974976,
      "learning_rate": 9.799196787148594e-06,
      "loss": 2.5869,
      "step": 1140
    },
    {
      "epoch": 1.661849710982659,
      "grad_norm": 1.250667929649353,
      "learning_rate": 9.397590361445783e-06,
      "loss": 2.5531,
      "step": 1150
    },
    {
      "epoch": 1.6763005780346822,
      "grad_norm": 1.42775297164917,
      "learning_rate": 8.995983935742972e-06,
      "loss": 2.5938,
      "step": 1160
    },
    {
      "epoch": 1.6907514450867052,
      "grad_norm": 1.2711819410324097,
      "learning_rate": 8.594377510040161e-06,
      "loss": 2.545,
      "step": 1170
    },
    {
      "epoch": 1.7052023121387283,
      "grad_norm": 1.2749913930892944,
      "learning_rate": 8.19277108433735e-06,
      "loss": 2.5888,
      "step": 1180
    },
    {
      "epoch": 1.7196531791907514,
      "grad_norm": 1.300438404083252,
      "learning_rate": 7.791164658634538e-06,
      "loss": 2.6074,
      "step": 1190
    },
    {
      "epoch": 1.7341040462427746,
      "grad_norm": 1.3017189502716064,
      "learning_rate": 7.389558232931727e-06,
      "loss": 2.5343,
      "step": 1200
    },
    {
      "epoch": 1.7485549132947977,
      "grad_norm": 1.43607759475708,
      "learning_rate": 6.987951807228917e-06,
      "loss": 2.5603,
      "step": 1210
    },
    {
      "epoch": 1.7630057803468207,
      "grad_norm": 1.292738437652588,
      "learning_rate": 6.586345381526104e-06,
      "loss": 2.6097,
      "step": 1220
    },
    {
      "epoch": 1.7774566473988438,
      "grad_norm": 1.4373846054077148,
      "learning_rate": 6.184738955823293e-06,
      "loss": 2.5749,
      "step": 1230
    },
    {
      "epoch": 1.791907514450867,
      "grad_norm": 1.2094963788986206,
      "learning_rate": 5.783132530120483e-06,
      "loss": 2.5283,
      "step": 1240
    },
    {
      "epoch": 1.80635838150289,
      "grad_norm": 1.3384995460510254,
      "learning_rate": 5.381526104417671e-06,
      "loss": 2.5731,
      "step": 1250
    },
    {
      "epoch": 1.8208092485549132,
      "grad_norm": 1.3509609699249268,
      "learning_rate": 4.979919678714859e-06,
      "loss": 2.5706,
      "step": 1260
    },
    {
      "epoch": 1.8352601156069364,
      "grad_norm": 1.4355109930038452,
      "learning_rate": 4.578313253012049e-06,
      "loss": 2.5487,
      "step": 1270
    },
    {
      "epoch": 1.8497109826589595,
      "grad_norm": 1.3049061298370361,
      "learning_rate": 4.176706827309238e-06,
      "loss": 2.5954,
      "step": 1280
    },
    {
      "epoch": 1.8641618497109826,
      "grad_norm": 1.244272232055664,
      "learning_rate": 3.775100401606426e-06,
      "loss": 2.5358,
      "step": 1290
    },
    {
      "epoch": 1.8786127167630058,
      "grad_norm": 1.4554516077041626,
      "learning_rate": 3.3734939759036146e-06,
      "loss": 2.5723,
      "step": 1300
    },
    {
      "epoch": 1.893063583815029,
      "grad_norm": 1.3969621658325195,
      "learning_rate": 2.9718875502008034e-06,
      "loss": 2.5424,
      "step": 1310
    },
    {
      "epoch": 1.907514450867052,
      "grad_norm": 1.2877315282821655,
      "learning_rate": 2.570281124497992e-06,
      "loss": 2.592,
      "step": 1320
    },
    {
      "epoch": 1.9219653179190752,
      "grad_norm": 1.249395489692688,
      "learning_rate": 2.168674698795181e-06,
      "loss": 2.515,
      "step": 1330
    },
    {
      "epoch": 1.9364161849710984,
      "grad_norm": 1.3880640268325806,
      "learning_rate": 1.7670682730923694e-06,
      "loss": 2.5687,
      "step": 1340
    },
    {
      "epoch": 1.9508670520231215,
      "grad_norm": 1.3549842834472656,
      "learning_rate": 1.3654618473895584e-06,
      "loss": 2.5667,
      "step": 1350
    },
    {
      "epoch": 1.9653179190751446,
      "grad_norm": 1.5668351650238037,
      "learning_rate": 9.638554216867472e-07,
      "loss": 2.584,
      "step": 1360
    },
    {
      "epoch": 1.9797687861271678,
      "grad_norm": 1.2866957187652588,
      "learning_rate": 5.622489959839358e-07,
      "loss": 2.5932,
      "step": 1370
    },
    {
      "epoch": 1.9942196531791907,
      "grad_norm": 1.144093632698059,
      "learning_rate": 1.606425702811245e-07,
      "loss": 2.5814,
      "step": 1380
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.38569974899292,
      "eval_runtime": 196.8635,
      "eval_samples_per_second": 5.1,
      "eval_steps_per_second": 0.64,
      "step": 1384
    }
  ],
  "logging_steps": 10,
  "max_steps": 1384,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 725514926948352.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
