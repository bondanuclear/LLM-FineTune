{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1384,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014450867052023121,
      "grad_norm": 0.4773590862751007,
      "learning_rate": 3.5971223021582732e-06,
      "loss": 3.7833,
      "step": 10
    },
    {
      "epoch": 0.028901734104046242,
      "grad_norm": 0.7151181101799011,
      "learning_rate": 7.1942446043165465e-06,
      "loss": 3.7712,
      "step": 20
    },
    {
      "epoch": 0.04335260115606936,
      "grad_norm": 0.6033282279968262,
      "learning_rate": 1.0791366906474821e-05,
      "loss": 3.7902,
      "step": 30
    },
    {
      "epoch": 0.057803468208092484,
      "grad_norm": 0.6800493001937866,
      "learning_rate": 1.4388489208633093e-05,
      "loss": 3.7653,
      "step": 40
    },
    {
      "epoch": 0.07225433526011561,
      "grad_norm": 0.7056021690368652,
      "learning_rate": 1.7985611510791367e-05,
      "loss": 3.701,
      "step": 50
    },
    {
      "epoch": 0.08670520231213873,
      "grad_norm": 0.8120033144950867,
      "learning_rate": 2.1582733812949642e-05,
      "loss": 3.8243,
      "step": 60
    },
    {
      "epoch": 0.10115606936416185,
      "grad_norm": 0.8941109776496887,
      "learning_rate": 2.5179856115107914e-05,
      "loss": 3.7716,
      "step": 70
    },
    {
      "epoch": 0.11560693641618497,
      "grad_norm": 0.9626595377922058,
      "learning_rate": 2.8776978417266186e-05,
      "loss": 3.7057,
      "step": 80
    },
    {
      "epoch": 0.13005780346820808,
      "grad_norm": 1.2160077095031738,
      "learning_rate": 3.237410071942446e-05,
      "loss": 3.6895,
      "step": 90
    },
    {
      "epoch": 0.14450867052023122,
      "grad_norm": 1.5225915908813477,
      "learning_rate": 3.597122302158273e-05,
      "loss": 3.6631,
      "step": 100
    },
    {
      "epoch": 0.15895953757225434,
      "grad_norm": 0.9782919883728027,
      "learning_rate": 3.956834532374101e-05,
      "loss": 3.4846,
      "step": 110
    },
    {
      "epoch": 0.17341040462427745,
      "grad_norm": 1.3269627094268799,
      "learning_rate": 4.3165467625899284e-05,
      "loss": 3.5845,
      "step": 120
    },
    {
      "epoch": 0.18786127167630057,
      "grad_norm": 1.3205915689468384,
      "learning_rate": 4.676258992805755e-05,
      "loss": 3.4419,
      "step": 130
    },
    {
      "epoch": 0.2023121387283237,
      "grad_norm": 1.314456820487976,
      "learning_rate": 4.995983935742972e-05,
      "loss": 3.475,
      "step": 140
    },
    {
      "epoch": 0.21676300578034682,
      "grad_norm": 1.125549077987671,
      "learning_rate": 4.955823293172691e-05,
      "loss": 3.411,
      "step": 150
    },
    {
      "epoch": 0.23121387283236994,
      "grad_norm": 0.982186496257782,
      "learning_rate": 4.9156626506024104e-05,
      "loss": 3.3526,
      "step": 160
    },
    {
      "epoch": 0.24566473988439305,
      "grad_norm": 0.9746118783950806,
      "learning_rate": 4.875502008032129e-05,
      "loss": 3.2781,
      "step": 170
    },
    {
      "epoch": 0.26011560693641617,
      "grad_norm": 1.026100516319275,
      "learning_rate": 4.8353413654618476e-05,
      "loss": 3.3099,
      "step": 180
    },
    {
      "epoch": 0.2745664739884393,
      "grad_norm": 0.9269070029258728,
      "learning_rate": 4.7951807228915665e-05,
      "loss": 3.2404,
      "step": 190
    },
    {
      "epoch": 0.28901734104046245,
      "grad_norm": 0.8253796696662903,
      "learning_rate": 4.7550200803212854e-05,
      "loss": 3.1995,
      "step": 200
    },
    {
      "epoch": 0.30346820809248554,
      "grad_norm": 0.8523250222206116,
      "learning_rate": 4.714859437751004e-05,
      "loss": 3.2033,
      "step": 210
    },
    {
      "epoch": 0.3179190751445087,
      "grad_norm": 0.7683552503585815,
      "learning_rate": 4.674698795180723e-05,
      "loss": 3.0935,
      "step": 220
    },
    {
      "epoch": 0.33236994219653176,
      "grad_norm": 0.7244157791137695,
      "learning_rate": 4.634538152610442e-05,
      "loss": 3.1444,
      "step": 230
    },
    {
      "epoch": 0.3468208092485549,
      "grad_norm": 0.8366519212722778,
      "learning_rate": 4.594377510040161e-05,
      "loss": 3.0686,
      "step": 240
    },
    {
      "epoch": 0.36127167630057805,
      "grad_norm": 0.8437788486480713,
      "learning_rate": 4.55421686746988e-05,
      "loss": 3.031,
      "step": 250
    },
    {
      "epoch": 0.37572254335260113,
      "grad_norm": 0.8275673389434814,
      "learning_rate": 4.514056224899599e-05,
      "loss": 3.1216,
      "step": 260
    },
    {
      "epoch": 0.3901734104046243,
      "grad_norm": 0.8020057678222656,
      "learning_rate": 4.473895582329318e-05,
      "loss": 3.0533,
      "step": 270
    },
    {
      "epoch": 0.4046242774566474,
      "grad_norm": 0.814189612865448,
      "learning_rate": 4.433734939759036e-05,
      "loss": 2.9959,
      "step": 280
    },
    {
      "epoch": 0.4190751445086705,
      "grad_norm": 0.8557850122451782,
      "learning_rate": 4.393574297188755e-05,
      "loss": 2.9353,
      "step": 290
    },
    {
      "epoch": 0.43352601156069365,
      "grad_norm": 0.8782809376716614,
      "learning_rate": 4.353413654618474e-05,
      "loss": 3.0285,
      "step": 300
    },
    {
      "epoch": 0.4479768786127168,
      "grad_norm": 0.9386831521987915,
      "learning_rate": 4.313253012048193e-05,
      "loss": 3.0343,
      "step": 310
    },
    {
      "epoch": 0.4624277456647399,
      "grad_norm": 0.9434203505516052,
      "learning_rate": 4.273092369477912e-05,
      "loss": 2.9556,
      "step": 320
    },
    {
      "epoch": 0.476878612716763,
      "grad_norm": 0.9923467636108398,
      "learning_rate": 4.232931726907631e-05,
      "loss": 2.8521,
      "step": 330
    },
    {
      "epoch": 0.4913294797687861,
      "grad_norm": 0.9191104769706726,
      "learning_rate": 4.1927710843373496e-05,
      "loss": 2.9503,
      "step": 340
    },
    {
      "epoch": 0.5057803468208093,
      "grad_norm": 0.9750847816467285,
      "learning_rate": 4.1526104417670686e-05,
      "loss": 2.9179,
      "step": 350
    },
    {
      "epoch": 0.5202312138728323,
      "grad_norm": 1.0308159589767456,
      "learning_rate": 4.1124497991967875e-05,
      "loss": 2.8468,
      "step": 360
    },
    {
      "epoch": 0.5346820809248555,
      "grad_norm": 1.0001957416534424,
      "learning_rate": 4.0722891566265064e-05,
      "loss": 2.9341,
      "step": 370
    },
    {
      "epoch": 0.5491329479768786,
      "grad_norm": 0.9109511971473694,
      "learning_rate": 4.0321285140562246e-05,
      "loss": 2.8258,
      "step": 380
    },
    {
      "epoch": 0.5635838150289018,
      "grad_norm": 0.9348042607307434,
      "learning_rate": 3.9919678714859436e-05,
      "loss": 2.8792,
      "step": 390
    },
    {
      "epoch": 0.5780346820809249,
      "grad_norm": 1.0832616090774536,
      "learning_rate": 3.9518072289156625e-05,
      "loss": 2.9001,
      "step": 400
    },
    {
      "epoch": 0.5924855491329479,
      "grad_norm": 1.0147678852081299,
      "learning_rate": 3.9116465863453814e-05,
      "loss": 2.8633,
      "step": 410
    },
    {
      "epoch": 0.6069364161849711,
      "grad_norm": 1.1274040937423706,
      "learning_rate": 3.8714859437751e-05,
      "loss": 2.8038,
      "step": 420
    },
    {
      "epoch": 0.6213872832369942,
      "grad_norm": 1.0238709449768066,
      "learning_rate": 3.831325301204819e-05,
      "loss": 2.8706,
      "step": 430
    },
    {
      "epoch": 0.6358381502890174,
      "grad_norm": 1.0547271966934204,
      "learning_rate": 3.791164658634538e-05,
      "loss": 2.7952,
      "step": 440
    },
    {
      "epoch": 0.6502890173410405,
      "grad_norm": 1.245968222618103,
      "learning_rate": 3.751004016064257e-05,
      "loss": 2.8471,
      "step": 450
    },
    {
      "epoch": 0.6647398843930635,
      "grad_norm": 1.252712607383728,
      "learning_rate": 3.710843373493976e-05,
      "loss": 2.8297,
      "step": 460
    },
    {
      "epoch": 0.6791907514450867,
      "grad_norm": 1.0350323915481567,
      "learning_rate": 3.670682730923695e-05,
      "loss": 2.7751,
      "step": 470
    },
    {
      "epoch": 0.6936416184971098,
      "grad_norm": 1.0472110509872437,
      "learning_rate": 3.630522088353414e-05,
      "loss": 2.7636,
      "step": 480
    },
    {
      "epoch": 0.708092485549133,
      "grad_norm": 0.9465208053588867,
      "learning_rate": 3.590361445783133e-05,
      "loss": 2.7261,
      "step": 490
    },
    {
      "epoch": 0.7225433526011561,
      "grad_norm": 0.9796590805053711,
      "learning_rate": 3.550200803212852e-05,
      "loss": 2.7711,
      "step": 500
    },
    {
      "epoch": 0.7369942196531792,
      "grad_norm": 1.0718580484390259,
      "learning_rate": 3.5100401606425706e-05,
      "loss": 2.7691,
      "step": 510
    },
    {
      "epoch": 0.7514450867052023,
      "grad_norm": 1.0348858833312988,
      "learning_rate": 3.4698795180722896e-05,
      "loss": 2.6807,
      "step": 520
    },
    {
      "epoch": 0.7658959537572254,
      "grad_norm": 1.0460807085037231,
      "learning_rate": 3.4297188755020085e-05,
      "loss": 2.7566,
      "step": 530
    },
    {
      "epoch": 0.7803468208092486,
      "grad_norm": 1.0998892784118652,
      "learning_rate": 3.3895582329317274e-05,
      "loss": 2.7472,
      "step": 540
    },
    {
      "epoch": 0.7947976878612717,
      "grad_norm": 1.1205788850784302,
      "learning_rate": 3.349397590361446e-05,
      "loss": 2.8118,
      "step": 550
    },
    {
      "epoch": 0.8092485549132948,
      "grad_norm": 1.1423025131225586,
      "learning_rate": 3.309236947791165e-05,
      "loss": 2.7124,
      "step": 560
    },
    {
      "epoch": 0.8236994219653179,
      "grad_norm": 1.0714898109436035,
      "learning_rate": 3.269076305220884e-05,
      "loss": 2.7108,
      "step": 570
    },
    {
      "epoch": 0.838150289017341,
      "grad_norm": 1.0902395248413086,
      "learning_rate": 3.2289156626506024e-05,
      "loss": 2.7792,
      "step": 580
    },
    {
      "epoch": 0.8526011560693642,
      "grad_norm": 1.0998562574386597,
      "learning_rate": 3.1887550200803213e-05,
      "loss": 2.7199,
      "step": 590
    },
    {
      "epoch": 0.8670520231213873,
      "grad_norm": 1.078199863433838,
      "learning_rate": 3.14859437751004e-05,
      "loss": 2.7217,
      "step": 600
    },
    {
      "epoch": 0.8815028901734104,
      "grad_norm": 1.1076446771621704,
      "learning_rate": 3.108433734939759e-05,
      "loss": 2.7093,
      "step": 610
    },
    {
      "epoch": 0.8959537572254336,
      "grad_norm": 1.030458927154541,
      "learning_rate": 3.068273092369478e-05,
      "loss": 2.6589,
      "step": 620
    },
    {
      "epoch": 0.9104046242774566,
      "grad_norm": 1.1098164319992065,
      "learning_rate": 3.028112449799197e-05,
      "loss": 2.6674,
      "step": 630
    },
    {
      "epoch": 0.9248554913294798,
      "grad_norm": 1.0295435190200806,
      "learning_rate": 2.987951807228916e-05,
      "loss": 2.6642,
      "step": 640
    },
    {
      "epoch": 0.9393063583815029,
      "grad_norm": 1.1246305704116821,
      "learning_rate": 2.947791164658635e-05,
      "loss": 2.661,
      "step": 650
    },
    {
      "epoch": 0.953757225433526,
      "grad_norm": 1.2026535272598267,
      "learning_rate": 2.9076305220883538e-05,
      "loss": 2.6114,
      "step": 660
    },
    {
      "epoch": 0.9682080924855492,
      "grad_norm": 1.1663471460342407,
      "learning_rate": 2.8674698795180727e-05,
      "loss": 2.6548,
      "step": 670
    },
    {
      "epoch": 0.9826589595375722,
      "grad_norm": 1.3090256452560425,
      "learning_rate": 2.827309236947791e-05,
      "loss": 2.6253,
      "step": 680
    },
    {
      "epoch": 0.9971098265895953,
      "grad_norm": 1.2640045881271362,
      "learning_rate": 2.78714859437751e-05,
      "loss": 2.7035,
      "step": 690
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.484206438064575,
      "eval_runtime": 177.8843,
      "eval_samples_per_second": 5.644,
      "eval_steps_per_second": 0.708,
      "step": 692
    },
    {
      "epoch": 1.0115606936416186,
      "grad_norm": 1.1440479755401611,
      "learning_rate": 2.7469879518072288e-05,
      "loss": 2.6556,
      "step": 700
    },
    {
      "epoch": 1.0260115606936415,
      "grad_norm": 1.3146322965621948,
      "learning_rate": 2.7068273092369477e-05,
      "loss": 2.6607,
      "step": 710
    },
    {
      "epoch": 1.0404624277456647,
      "grad_norm": 1.1217949390411377,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 2.6449,
      "step": 720
    },
    {
      "epoch": 1.0549132947976878,
      "grad_norm": 1.1914408206939697,
      "learning_rate": 2.6265060240963856e-05,
      "loss": 2.6546,
      "step": 730
    },
    {
      "epoch": 1.069364161849711,
      "grad_norm": 1.2775206565856934,
      "learning_rate": 2.5863453815261045e-05,
      "loss": 2.6394,
      "step": 740
    },
    {
      "epoch": 1.083815028901734,
      "grad_norm": 1.1788885593414307,
      "learning_rate": 2.5461847389558234e-05,
      "loss": 2.7473,
      "step": 750
    },
    {
      "epoch": 1.0982658959537572,
      "grad_norm": 1.27798593044281,
      "learning_rate": 2.5060240963855423e-05,
      "loss": 2.6277,
      "step": 760
    },
    {
      "epoch": 1.1127167630057804,
      "grad_norm": 1.4533559083938599,
      "learning_rate": 2.4658634538152613e-05,
      "loss": 2.568,
      "step": 770
    },
    {
      "epoch": 1.1271676300578035,
      "grad_norm": 1.2459166049957275,
      "learning_rate": 2.4257028112449802e-05,
      "loss": 2.5919,
      "step": 780
    },
    {
      "epoch": 1.1416184971098267,
      "grad_norm": 1.266132116317749,
      "learning_rate": 2.385542168674699e-05,
      "loss": 2.6984,
      "step": 790
    },
    {
      "epoch": 1.1560693641618498,
      "grad_norm": 1.2821277379989624,
      "learning_rate": 2.345381526104418e-05,
      "loss": 2.6137,
      "step": 800
    },
    {
      "epoch": 1.1705202312138727,
      "grad_norm": 1.34568452835083,
      "learning_rate": 2.3052208835341366e-05,
      "loss": 2.6336,
      "step": 810
    },
    {
      "epoch": 1.1849710982658959,
      "grad_norm": 1.4371408224105835,
      "learning_rate": 2.2650602409638555e-05,
      "loss": 2.6513,
      "step": 820
    },
    {
      "epoch": 1.199421965317919,
      "grad_norm": 1.159591555595398,
      "learning_rate": 2.2248995983935745e-05,
      "loss": 2.6254,
      "step": 830
    },
    {
      "epoch": 1.2138728323699421,
      "grad_norm": 1.1199257373809814,
      "learning_rate": 2.1847389558232934e-05,
      "loss": 2.6211,
      "step": 840
    },
    {
      "epoch": 1.2283236994219653,
      "grad_norm": 1.223948359489441,
      "learning_rate": 2.1445783132530123e-05,
      "loss": 2.6542,
      "step": 850
    },
    {
      "epoch": 1.2427745664739884,
      "grad_norm": 1.1453585624694824,
      "learning_rate": 2.104417670682731e-05,
      "loss": 2.587,
      "step": 860
    },
    {
      "epoch": 1.2572254335260116,
      "grad_norm": 1.1927673816680908,
      "learning_rate": 2.0642570281124498e-05,
      "loss": 2.6087,
      "step": 870
    },
    {
      "epoch": 1.2716763005780347,
      "grad_norm": 1.0940370559692383,
      "learning_rate": 2.0240963855421687e-05,
      "loss": 2.6556,
      "step": 880
    },
    {
      "epoch": 1.2861271676300579,
      "grad_norm": 1.4075720310211182,
      "learning_rate": 1.9839357429718877e-05,
      "loss": 2.6746,
      "step": 890
    },
    {
      "epoch": 1.300578034682081,
      "grad_norm": 1.1793042421340942,
      "learning_rate": 1.9437751004016066e-05,
      "loss": 2.6255,
      "step": 900
    },
    {
      "epoch": 1.3150289017341041,
      "grad_norm": 1.339765191078186,
      "learning_rate": 1.9036144578313252e-05,
      "loss": 2.5215,
      "step": 910
    },
    {
      "epoch": 1.3294797687861273,
      "grad_norm": 1.2710192203521729,
      "learning_rate": 1.863453815261044e-05,
      "loss": 2.6115,
      "step": 920
    },
    {
      "epoch": 1.3439306358381504,
      "grad_norm": 1.2933363914489746,
      "learning_rate": 1.823293172690763e-05,
      "loss": 2.5948,
      "step": 930
    },
    {
      "epoch": 1.3583815028901733,
      "grad_norm": 1.3007146120071411,
      "learning_rate": 1.783132530120482e-05,
      "loss": 2.5523,
      "step": 940
    },
    {
      "epoch": 1.3728323699421965,
      "grad_norm": 1.399218201637268,
      "learning_rate": 1.742971887550201e-05,
      "loss": 2.5801,
      "step": 950
    },
    {
      "epoch": 1.3872832369942196,
      "grad_norm": 1.2420756816864014,
      "learning_rate": 1.7028112449799198e-05,
      "loss": 2.5834,
      "step": 960
    },
    {
      "epoch": 1.4017341040462428,
      "grad_norm": 1.2903567552566528,
      "learning_rate": 1.6626506024096387e-05,
      "loss": 2.5641,
      "step": 970
    },
    {
      "epoch": 1.416184971098266,
      "grad_norm": 1.3577089309692383,
      "learning_rate": 1.6224899598393576e-05,
      "loss": 2.5448,
      "step": 980
    },
    {
      "epoch": 1.430635838150289,
      "grad_norm": 1.3114948272705078,
      "learning_rate": 1.5823293172690766e-05,
      "loss": 2.6109,
      "step": 990
    },
    {
      "epoch": 1.4450867052023122,
      "grad_norm": 1.3202382326126099,
      "learning_rate": 1.5421686746987955e-05,
      "loss": 2.611,
      "step": 1000
    },
    {
      "epoch": 1.4595375722543353,
      "grad_norm": 1.2670037746429443,
      "learning_rate": 1.502008032128514e-05,
      "loss": 2.5911,
      "step": 1010
    },
    {
      "epoch": 1.4739884393063583,
      "grad_norm": 1.280859112739563,
      "learning_rate": 1.461847389558233e-05,
      "loss": 2.6318,
      "step": 1020
    },
    {
      "epoch": 1.4884393063583814,
      "grad_norm": 1.2268590927124023,
      "learning_rate": 1.4216867469879519e-05,
      "loss": 2.5741,
      "step": 1030
    },
    {
      "epoch": 1.5028901734104045,
      "grad_norm": 1.1395704746246338,
      "learning_rate": 1.3815261044176708e-05,
      "loss": 2.497,
      "step": 1040
    },
    {
      "epoch": 1.5173410404624277,
      "grad_norm": 1.2180790901184082,
      "learning_rate": 1.3413654618473897e-05,
      "loss": 2.5654,
      "step": 1050
    },
    {
      "epoch": 1.5317919075144508,
      "grad_norm": 1.1662214994430542,
      "learning_rate": 1.3012048192771083e-05,
      "loss": 2.5491,
      "step": 1060
    },
    {
      "epoch": 1.546242774566474,
      "grad_norm": 1.2439628839492798,
      "learning_rate": 1.2610441767068274e-05,
      "loss": 2.5821,
      "step": 1070
    },
    {
      "epoch": 1.560693641618497,
      "grad_norm": 1.1463375091552734,
      "learning_rate": 1.2208835341365463e-05,
      "loss": 2.529,
      "step": 1080
    },
    {
      "epoch": 1.5751445086705202,
      "grad_norm": 1.3610503673553467,
      "learning_rate": 1.1807228915662651e-05,
      "loss": 2.5615,
      "step": 1090
    },
    {
      "epoch": 1.5895953757225434,
      "grad_norm": 1.3866050243377686,
      "learning_rate": 1.140562248995984e-05,
      "loss": 2.5783,
      "step": 1100
    },
    {
      "epoch": 1.6040462427745665,
      "grad_norm": 1.3864508867263794,
      "learning_rate": 1.100401606425703e-05,
      "loss": 2.6187,
      "step": 1110
    },
    {
      "epoch": 1.6184971098265897,
      "grad_norm": 1.234043002128601,
      "learning_rate": 1.0602409638554217e-05,
      "loss": 2.5769,
      "step": 1120
    },
    {
      "epoch": 1.6329479768786128,
      "grad_norm": 1.1170400381088257,
      "learning_rate": 1.0200803212851406e-05,
      "loss": 2.5618,
      "step": 1130
    },
    {
      "epoch": 1.647398843930636,
      "grad_norm": 1.3208389282226562,
      "learning_rate": 9.799196787148594e-06,
      "loss": 2.5824,
      "step": 1140
    },
    {
      "epoch": 1.661849710982659,
      "grad_norm": 1.258543610572815,
      "learning_rate": 9.397590361445783e-06,
      "loss": 2.5488,
      "step": 1150
    },
    {
      "epoch": 1.6763005780346822,
      "grad_norm": 1.385183334350586,
      "learning_rate": 8.995983935742972e-06,
      "loss": 2.5901,
      "step": 1160
    },
    {
      "epoch": 1.6907514450867052,
      "grad_norm": 1.2507050037384033,
      "learning_rate": 8.594377510040161e-06,
      "loss": 2.5404,
      "step": 1170
    },
    {
      "epoch": 1.7052023121387283,
      "grad_norm": 1.2529716491699219,
      "learning_rate": 8.19277108433735e-06,
      "loss": 2.5844,
      "step": 1180
    },
    {
      "epoch": 1.7196531791907514,
      "grad_norm": 1.2544853687286377,
      "learning_rate": 7.791164658634538e-06,
      "loss": 2.6056,
      "step": 1190
    },
    {
      "epoch": 1.7341040462427746,
      "grad_norm": 1.2854808568954468,
      "learning_rate": 7.389558232931727e-06,
      "loss": 2.5289,
      "step": 1200
    },
    {
      "epoch": 1.7485549132947977,
      "grad_norm": 1.4294474124908447,
      "learning_rate": 6.987951807228917e-06,
      "loss": 2.5561,
      "step": 1210
    },
    {
      "epoch": 1.7630057803468207,
      "grad_norm": 1.2534414529800415,
      "learning_rate": 6.586345381526104e-06,
      "loss": 2.6081,
      "step": 1220
    },
    {
      "epoch": 1.7774566473988438,
      "grad_norm": 1.4225951433181763,
      "learning_rate": 6.184738955823293e-06,
      "loss": 2.5699,
      "step": 1230
    },
    {
      "epoch": 1.791907514450867,
      "grad_norm": 1.1738044023513794,
      "learning_rate": 5.783132530120483e-06,
      "loss": 2.5243,
      "step": 1240
    },
    {
      "epoch": 1.80635838150289,
      "grad_norm": 1.3297507762908936,
      "learning_rate": 5.381526104417671e-06,
      "loss": 2.5673,
      "step": 1250
    },
    {
      "epoch": 1.8208092485549132,
      "grad_norm": 1.3009699583053589,
      "learning_rate": 4.979919678714859e-06,
      "loss": 2.5666,
      "step": 1260
    },
    {
      "epoch": 1.8352601156069364,
      "grad_norm": 1.416426420211792,
      "learning_rate": 4.578313253012049e-06,
      "loss": 2.5435,
      "step": 1270
    },
    {
      "epoch": 1.8497109826589595,
      "grad_norm": 1.3212413787841797,
      "learning_rate": 4.176706827309238e-06,
      "loss": 2.5909,
      "step": 1280
    },
    {
      "epoch": 1.8641618497109826,
      "grad_norm": 1.2327823638916016,
      "learning_rate": 3.775100401606426e-06,
      "loss": 2.5301,
      "step": 1290
    },
    {
      "epoch": 1.8786127167630058,
      "grad_norm": 1.4040496349334717,
      "learning_rate": 3.3734939759036146e-06,
      "loss": 2.5672,
      "step": 1300
    },
    {
      "epoch": 1.893063583815029,
      "grad_norm": 1.4357391595840454,
      "learning_rate": 2.9718875502008034e-06,
      "loss": 2.537,
      "step": 1310
    },
    {
      "epoch": 1.907514450867052,
      "grad_norm": 1.3015415668487549,
      "learning_rate": 2.570281124497992e-06,
      "loss": 2.5867,
      "step": 1320
    },
    {
      "epoch": 1.9219653179190752,
      "grad_norm": 1.2264645099639893,
      "learning_rate": 2.168674698795181e-06,
      "loss": 2.5108,
      "step": 1330
    },
    {
      "epoch": 1.9364161849710984,
      "grad_norm": 1.372060775756836,
      "learning_rate": 1.7670682730923694e-06,
      "loss": 2.5647,
      "step": 1340
    },
    {
      "epoch": 1.9508670520231215,
      "grad_norm": 1.2995187044143677,
      "learning_rate": 1.3654618473895584e-06,
      "loss": 2.56,
      "step": 1350
    },
    {
      "epoch": 1.9653179190751446,
      "grad_norm": 1.528687596321106,
      "learning_rate": 9.638554216867472e-07,
      "loss": 2.5796,
      "step": 1360
    },
    {
      "epoch": 1.9797687861271678,
      "grad_norm": 1.3000884056091309,
      "learning_rate": 5.622489959839358e-07,
      "loss": 2.5855,
      "step": 1370
    },
    {
      "epoch": 1.9942196531791907,
      "grad_norm": 1.1294792890548706,
      "learning_rate": 1.606425702811245e-07,
      "loss": 2.576,
      "step": 1380
    },
    {
      "epoch": 2.0,
      "eval_loss": 2.3820748329162598,
      "eval_runtime": 172.8892,
      "eval_samples_per_second": 5.807,
      "eval_steps_per_second": 0.729,
      "step": 1384
    }
  ],
  "logging_steps": 10,
  "max_steps": 1384,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 725514926948352.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
